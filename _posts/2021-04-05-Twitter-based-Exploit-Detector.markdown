---
layout: single
title:  "Twitter-based Exploit Detector"
date:   2021-04-05 16:54:50 -0500
categories: app-development
tags: machine-learning awae
permalink: /:categories/:title/

header:
  overlay_image: /images/cloudflare1/banner.png
  overlay_filter: rgba(0, 0, 0, 0.7)
  actions:
     - label: "View Code"
       url: "https://github.com/alexfrancow/TED"
  
---

TED aka Twitter-based Exploit Detector es un bot en Twitter que nos informará cuando se publique una prueba de concepto o exploit en Twitter, síguelo en [@alexfrancow_sec](https://twitter.com/alexfrancow_sec)


## Introducción

Hola a todos y bienvenidos al segundo post de mi blog, hace aproximadamente unos 3 meses estuve desarrollando una aplicación que me permitia sacar todos los tweets, repositorios y enlaces que contuviesen un determinado CVE mostrando con gráficas; recuentos de tweets, retweets y likes, con esos datos y gracias a los algoritmos de Machine Learning podía identificar que tweets contenian una posible prueba de concepto/exploit, dejo el enlace de YouTube: [NLP to detect Exploits on Twitter](https://www.youtube.com/watch?v=bsBroYeO0e8)

Debido a que actualmente me está siendo muy dificil sacar tiempo para poder continuar con el desarrollo de la aplicación, en esta semana santa (2 días) se me ocurrió que podía simplificarlo mucho más y tener algo provisional para que me alertase de cuando se publicasen pruebas de concepto de exploits en Twitter, a continuación os detallo el desarrollo de TED.


## API de Twitter

Si tenéis twitter y estáis en el mundo de la ciberseguridad, sabréis que es una plataforma muy importante, ya que te mantiene actualizado en todo momento de nuevas vulnerabilidades, nuevos exploits, nuevos zero-days o nuevas amenazas que surgen en el mundo. Siguiendo con el tema principal de la aplicación, mi idea fue la siguiente; crear un bot que hiciese retweets a los tweets que tuviesen una prueba de concepto o exploit para de esta manera activando la campana en ese perfil me alertase de cuando se publicase algo, asi todo el mundo podría seguir al bot y mantenerse informado también.

El bot ataca a varios endpoints de la API de twitter, uno de ellos es donde se realiza un streaming de todos los tweets, pero para consumir de dicha API es necesario tener una cuenta de desarrollador y para conseguirla es necesario enviar un email mencionando los intereses en utilizar la API.

<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/TED/main/images/1.PNG" height="500" width="825" /></p>
<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/TED/main/images/2.PNG" height="500" width="825" /></p>
<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/TED/main/images/3.PNG" height="500" width="825" /></p>

Una vez aprobada por el equipo de Twitter, en el panel de desarrollador: ```https://developer.twitter.com/en/portal/dashboard``` saldrá el proyecto que nos han creado, en este caso para propósitos académicos se habilitará el acceso a la V1.1 y a la V2 de la API.

<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/TED/main/images/4.PNG" height="500" width="825" /></p>

> Desde el mismo panel es donde se generan los tokens (API Key & Secret y el Bearer Token) que se utilizarán para la aplicación en Python.


## Generación del Dataset y creación del Modelo

Teniendo ya acceso a la API de Twitter solo quedaría entrenar los algoritmos correspondientes que hacen que se pueda detectar cuando un Tweet se trata de una PoC/Exploit y cuando no, para ello se descargarán todos los tweets que contengan "CVE-" de los años 2019-2020 y se pasaran a .csv, existe una app llamada [twint](https://github.com/twintproject/twint) desarrollada en python que hará por nosotros este trabajo:

```bash
# Get old tweets
https://github.com/twintproject/twint

conda install -c anaconda git 
pip3 install twint
pip3 install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint

# Collect Tweets that were tweeted before 2020.
twint -s "cve-" --year 2020 -o cves-2019.csv --csv

# Collect Tweets that were tweeted since 2015-12-20 00:00:00.
twint -s "CVE-" --since 2015-12-20
```

Una vez descargados, se necesita identificar que tweet es una PoC/Exploit y cual no, un proceso manual que se basa en abrir el .csv e ir linea por linea marcando 1 o 0 en función si vemos que se trata de un exploit o no, esto es necesario para el algoritmo de clasificación que vamos a utilizar ya que al ser supervisado tenemos que clasificar nosotros el dataset poniendo etiquetas (labeled data), en este caso la etiqueta 'exploit' que será igual a 1 o 0.

<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/TED/main/images/5.PNG" height="500" width="825" /></p>

Para esta aplicación se clasificaron muy pocos tweets debido al tiempo de desarrollo, en total fueron 629. Una vez generado el dataset el modelo fue entrenando siguiendo el algoritmo MultinomialNB, un tipo de algoritmo del conjunto Naive Bayes, una clase especial de algoritmos de clasificación de Machine Learning que se basan en una técnica de clasificación estadística llamada “teorema de Bayes”, como se mencionó anteriormente supervisado.

Estos modelos son llamados algoritmos “Naive” y en ellos se asume que las variables predictoras son independientes entre sí. En otras palabras, que la presencia de una cierta característica en un conjunto de datos no está en absoluto relacionada con la presencia de cualquier otra característica.

Un claro ejemplo sería:

Consideremos el caso de dos compañeros que trabajan en la misma oficina: Alicia y Bruno. Sabemos que:

    Alicia viene a la oficina 3 días a la semana.
    Bruno viene a la oficina 1 día a la semana.

Esta sería nuestra información “anterior”.

Estamos en la oficina y vemos pasar delante de nosotros a alguien muy rápido, tan rápido que no sabemos si es Alicia o Bruno.

Dada la información que tenemos hasta ahora y asumiendo que solo trabajan 4 días a la semana, las probabilidades de que la persona vista sea Alicia o Bruno, son:

    P(Alicia) = 3/4 = 0.75
    P(Bruno) = 1/4 = 0.25

Cuando vimos a la persona pasar, vimos que él o ella llevaba una chaqueta roja. También sabemos lo siguiente:

    Alicia viste de rojo 2 veces a la semana.
    Bruno viste de rojo 3 veces a la semana.

Así que, para cada semana de trabajo, que tiene cinco días, podemos inferir lo siguiente:

    La probabilidad de que Alicia vista de rojo es → P(Rojo|Alicia) = 2/5 = 0.4
    La probabilidad de que Bruno vista de rojo → P(Rojo|Bruno) = 3/5 = 0.6

Entonces, con esta información, ¿a quién vimos pasar? (en forma de probabilidad), esta nueva probabilidad será la información ‘posterior’.

En este caso usaremos la implementación Multinomial Naive Bayes “multinomialNB”. MultinomialNB implementa el algoritmo para datos distribuidos multinomialmente y es una de las dos variantes clásicas que se utilizan en la clasificación de texto (donde los datos se representan normalmente como recuentos de vectores de palabras, aunque también se sabe que los vectores tf-idf funcionan bien en la práctica). La distribución está parametrizada por vectores <math xmlns="http://www.w3.org/1998/Math/MathML">  <msub>    <mi>&#x3B8;</mi>    <mi>y</mi>  </msub>  <mo>=</mo>  <mo stretchy="false">(</mo>  <msub>    <mi>&#x3B8;</mi>    <mrow>      <mi>y</mi>      <mn>1</mn>    </mrow>  </msub>  <mo>,</mo>  <mo>&#x2026;</mo>  <mo>,</mo>  <msub>    <mi>&#x3B8;</mi>    <mrow>      <mi>y</mi>      <mi>n</mi>    </mrow>  </msub>  <mo stretchy="false">)</mo></math> para cada clase <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math>, donde <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math> es el número de características (en la clasificación del texto, el tamaño del vocabulario) y <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x3B8;</mi><mrow><mi>y</mi><mi>i</mi></mrow></msub></math> es la probabilidad <math xmlns="http://www.w3.org/1998/Math/MathML">  <mi>P</mi>  <mo stretchy="false">(</mo>  <msub>    <mi>x</mi>    <mi>i</mi>  </msub>  <mo>&#x2223;</mo>  <mi>y</mi>  <mo stretchy="false">)</mo></math> de característica <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi></math> que aparece en una muestra perteneciente a la clase <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math>. 

> [Algoritmo multinomialNB](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)

El primer paso para comenzar a utilizar el algoritmo descrito anteriormente será importar el dataset en el que se ha etiquetado cada tweet.

```python
import pandas as pd
pd.options.display.float_format = '{:.0f}'.format
columns = "id,date,username,tweet,exploit"

df = pd.read_csv('DATASET.csv', usecols=columns.split(","))  
```

Una vez importado llega la parte más importante a la hora de entrenar nuestro modelo, y es pre-procesar los datos; sustitución de URLs y nombres de usuario por palabras clave, eliminación de signos de puntuación y la conversión a minúsculas, y la normalización del texto (stemmer, lemmatize). Para ello se crearon las siguientes funciones:

```
def remove_URL(x):
    return re.sub(r"http\S+", "", x)

def tokenize(x):
    tokenizer = TweetTokenizer()
    return tokenizer.tokenize(x.lower())

def tokenize_remove_regex(x):
    listToStr = ' '.join([str(elem) for elem in x]) 
    tokenizer = RegexpTokenizer(r'http|2019|2018|cve|2020| |\.|,|:|;|!|\?|\(|\)|\||\+|\'|"|‘|’|“|”|\'|\’|…|\-|–|—|\$|&|\*|>|<|\/|\[|\]', gaps=True)
    return tokenizer.tokenize(listToStr)

def stemmer(x):
    stemmer = PorterStemmer()
    return ' '.join([stemmer.stem(word) for word in x])
 
def lemmatize(x):
    lemmatizer = WordNetLemmatizer()
    return ' '.join([lemmatizer.lemmatize(word) for word in x])
```

> Stemmer: es el proceso de producir variantes morfológicas de una palabra raíz/base. Un algoritmo de derivación reduce las palabras "chocolates", "chocolatey", "choco" a la raíz de la palabra, "chocolate" y "recuperación", "recuperado", "recupera" se reduce a la raíz "recuperar".

Aplicando esa serie de funciones quedaría un dataframe como el siguiente:

<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/TED/main/images/11.PNG" height="500" width="825" /></p>

Cuando se haya pre-procesado el dataset (parte más importante de ML), haremos el modelo de Machine Learning, que como se mencionaba anteriormente se trata de un ```MultinomialNB```, seleccionaremos X (tweet stemmer) e Y (categoria) y haremos un split de nuestro dataframe para dividir matrices de datos en dos subconjuntos: para entrenar datos y para probar datos. Con esta función, no es necesario dividir el conjunto de datos manualmente. 

```python
# Train/test
X = df['stems']
y = df['exploit']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 13)
```
> De forma predeterminada, Sklearn train_test_split hará particiones aleatorias para los dos subconjuntos.

Con el siguiente bloque de codigo pondremos a entrenar a nuestro modelo de ML, importaremos el clasificador “MultinomialNB” y ajustaremos los datos de entrenamiento en el clasificador usando fit().

```python
# Train bayes classifier MultinomialNB

pipe_mnnb = Pipeline(steps = [('tf', TfidfVectorizer()), ('mnnb', MultinomialNB())])

# Parameter grid
pgrid_mnnb = {
    'tf__max_features' : [1000, 2000, 3000],
    'tf__stop_words' : ['english', stop_words_alexfrancow],
    'tf__ngram_range' : [(1,1),(1,2)],
    'tf__use_idf' : [True, False],
    'mnnb__alpha' : [0.1, 0.5, 1]
}

gs_mnnb = GridSearchCV(pipe_mnnb, pgrid_mnnb, cv=5, n_jobs=-1)
gs_mnnb.fit(X_train, y_train)
```

Se aprecia que el entrenamiento con 629 tweets da un Accuracy de 0.77, el accuracy es la precisión de casos clasificados correctamente, si la exactitud es menor o igual al 50% el modelo no será útil ya que sería lanzar una moneda al aire para tomar decisiones. Para que sea considerado fiable el porcentaje debería ser un 90%:

```python
# Confusion matrix
import matplotlib.pyplot as plt
from sklearn.metrics import plot_confusion_matrix, accuracy_score

y_pred_class = gs_mnnb.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred_class))
plot_confusion_matrix(gs_mnnb, X_test, y_test)
plt.title('Confusion matrix of the classifier')
plt.show()
```

Como se puede apreciar en la matriz de confusión, la diagonal principal contiene la suma de todas las predicciones correctas, la otra diagonal refleja los errores del clasificador: los falsos positivos o los falsos negativos.

<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/TED/main/images/12.PNG" height="500" width="825" /></p>

Una vez entrenado el modelo simplemente quedaría por consumir de la API de Twitter y sacar los tweets en tiempo real, para ello se hará uso del siguiente endpoint: ```https://api.twitter.com/2/tweets/search/stream```, cabe recalcar que al no ser interesantes todos los tweets antes se deberá aplicar una regla y filtrar asi los tweets que contengan el string "CVE-", para ello se usará el endpoint: ```https://api.twitter.com/2/tweets/search/stream/rules``` el cual permitirá crear una regla que además omita los retweets y los replies, la consulta sería la siguiente:

```python
sample_rules = [
        {'value': '"CVE-" -is:retweet -is:reply',
         'tag': 'Exploits'},
    ]
payload = {"add": sample_rules}
response = requests.post(
        "https://api.twitter.com/2/tweets/search/stream/rules",
        headers=headers,
        json=payload,
    )
```

La documentación sobre el stream y las reglas podemos consultar los siguientes enlaces:

- Docu: https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/quick-start
- Rules: https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rule


<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/TED/main/images/6.PNG" height="500" width="825" /></p>


Llegados hasta aqui solo nos queda una pregunta por responder, ¿es funcional?

Pues si, el bot es funcional, pero todavía no es fiable ya que solo tiene un 77% de precisión por lo tanto el bot categoriza algunos tweets que no son exploits, precisión que se podrá mejorar; etiquetando más tweets del dataset y entrenando de nuevo el modelo o pre-procesando los datos de otra manera.


## PoC

De momento todo fue teorico, veamos un caso que ha considerado como exploit:

<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/TED/main/images/7.PNG" height="500" width="825" /></p>

Efectivamente es un exploit para la vulnerabilidad CVE-2020-25078 que afecta a las camaras D_Link-DCS-2530L, una vulnerabilidad vieja pero ejemplifica bien el funcionamiento del bot, si nos vamos a shodan.io y buscamos por ese modelo de camara nos aparecerán 4275 resultados, si entramos en una IP del listado y añadimos a la url "/config/getuser?index=0" veremos que en muchos casos es posible explotarlos, consiguiendo de esta manera las credenciales de la cuenta administrador que nos permiten entrar en la camara y ver en tiempo real el video capturado.

<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/TED/main/images/8.png" height="500" width="825" /></p>
<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/TED/main/images/9.png" height="500" width="825" /></p>
<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/TED/main/images/10.png" height="500" width="825" /></p>

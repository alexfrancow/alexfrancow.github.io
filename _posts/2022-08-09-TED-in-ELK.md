---
layout: single
title:  "TED in ELK"
date:   2022-08-09 16:54:50 -0500
categories: app-development
tags: machine-learning
permalink: /:categories/:title/

header:
  overlay_image: /images/cloudflare1/banner.png
  overlay_filter: rgba(0, 0, 0, 0.7)
  actions:
     - label: "View Code"
       url: "https://github.com/alexfrancow/ted-elk"
  
---

# TED in ELK
Hace ya un tiempo desarrollé una aplicación llamada **TED**, *más bien una PoC*, la cual era capaz de detectar *Exploits* y pruebas de concepto de vulnerabilidades en *tweets* a través de algoritmos de **Inteligencia Artificial** tales como el [multinomialNB](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) y la **API** de **Twitter**. 

> El articulo es posible encontrarlo en la siguiente entrada del blog: 
> - https://alexfrancow.github.io/app-development/Twitter-based-Exploit-Detector/
> - https://www.hackplayers.com/2021/04/ted-un-detector-de-exploits-en-twitter.html

![[Pasted image 20220907090803.png]]
![[Pasted image 20220905103420.png]]

Gracias a esta idea, se ha podido desarrollar un *conector* para **ELK**. Siendo apoyado por tecnologías *Open Source* como **Wazuh** las cuales permiten tener un listado de paquetes instalados en los agentes registrados en la plataforma, permitiendo mapear ese listado con bases de datos de **CVEs**, cuya finalidad es la de asociar **CVEs** a cada equipo. Esto se produce gracias al módulo `vulnerability-detector` de **Wazuh**. Este módulo es configurable a través del fichero `ossec.conf`.

```xml
      <vulnerability-detector>
        <enabled>yes</enabled>
        <interval>5m</interval>
        <ignore_time>6h</ignore_time>
        <run_on_start>yes</run_on_start>

        <!-- Ubuntu OS vulnerabilities -->
        <provider name="canonical">
          <enabled>no</enabled>
          <os>trusty</os>
          <os>xenial</os>
          <os>bionic</os>
          <os>focal</os>
          <update_interval>1h</update_interval>
        </provider>

        <!-- Debian OS vulnerabilities -->
        <provider name="debian">
          <enabled>no</enabled>
          <os>stretch</os>
          <os>buster</os>
          <update_interval>1h</update_interval>
        </provider>

        <!-- RedHat OS vulnerabilities -->
        <provider name="redhat">
          <enabled>no</enabled>
          <os>5</os>
          <os>6</os>
          <os>7</os>
          <os>8</os>
          <update_interval>1h</update_interval>
        </provider>

        <!-- Windows OS vulnerabilities -->
        <provider name="msu">
          <enabled>yes</enabled>
          <update_interval>1h</update_interval>
        </provider>

        <!-- Aggregate vulnerabilities -->
        <provider name="nvd">
          <enabled>yes</enabled>
          <update_from_year>2010</update_from_year>
          <update_interval>1h</update_interval>
        </provider>
      </vulnerability-detector>
```

> https://documentation.wazuh.com/current/user-manual/reference/ossec-conf/

Estos datos se almacenan en **OpenSearch/ElasticSearch**, por lo que desde cualquier programa podemos realizar consultas a la **API** y hacer uso de esos datos, en el caso de **TED**, realiza peticiones y almacena en un *array* todos los **CVEs** encontrados.

```python
cveList = WZ.getAllCves()
```

La lógica seguida para la aplicación se muestra en el siguiente diagrama.

<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/alexfrancow.github.io/master/images/2022-08-09-TED-in-ELK/Pasted image 20220905103757.png" height="500" width="825" /></p>

Como primer punto **TED** descarga el listado de **CVEs** que tienen afectación directa en la compañía donde se encuentre desplegada la solución. Con ese listado se mantiene escuchando el *stream* de **Twitter** en busca de posibles *exploits*, en caso de ser parseado por los algoritmos de **Inteligencia Artificial** y su resultado sea positivo, este hará un *bulk* del evento a la base de datos de **OpenSearch/ElasticSearch** y será consultado a través de unos dashboards de **Kibana** por un ingeniero, el cual dará prioridad a la hora de actualizar algún paquete, ya que habrá un incremento del riesgo relacionado con la vulnerabilidad.

## Despliegue
Se ha desarrollado una imagen en **Docker**, la cual incorpora las siguientes variables de entorno que son necesarias para el correcto funcionamiento de la aplicación, además de hacer la aplicación totalmente escalable en cualquier escenario.

-   `INDEXER_URL`: URL de OpenSearch/ElasticSearch.
-   `INDEXER_USERNAME`: Usuario de OpenSearch/ElasticSearch.
-   `INDEXER_PASSWORD`: Contraseña de OpenSearch/ElasticSearch.
-   `WAZUH_API_URL`: URL de Wazuh.
-   `API_USERNAME`: Usuario de la API de Wazuh.
-   `API_PASSWORD`: Contraseña de la API de Wazuh.
-   `BEARER_TOKEN`: Token de Twitter.
-   `TWITTER_APP_KEY`: Key de Twitter.
-   `TWITTER_APP_SECRET`: Secret de Twitter.
-   `ACCESS_TOKEN`: Access Token de Twitter.
-   `ACCESS_TOKEN_SECRET`: Secret del Access Token de Twitter.
-   `GET_ALL_TWS`: Variable de control para realizar solo búsquedas de CVEs encontrados por Wazuh. Esta variable es del tipo `bool`, el valor `False` para búscar solo CVEs detectados por Wazuh.

<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/alexfrancow.github.io/master/images/2022-08-09-TED-in-ELK/Pasted image 20220905115414.png" height="500" width="825" /></p>
<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/alexfrancow.github.io/master/images/2022-08-09-TED-in-ELK/Pasted image 20220907091716.png" height="500" width="825" /></p>

El despliegue es muy sencillo gracias a **Docker**, para desplegarlo simplemente es necesario ejecutar el siguiente comando: 

```bash
# Build
docker-compose build

# Deploy
docker run -e TWITTER_APP_KEY="" \
  -e TWITTER_APP_SECRET="" \
  -e access_token="" \
  -e access_token_secret="" \
  -e bearer_token="" \
  -e INDEXER_USERNAME="admin" \
  -e INDEXER_PASSWORD="" \
  -e GET_ALL_TWS="True" \
  -e INDEXER_URL="" \
  -e WAZUH_API_URL="" \
  -e API_USERNAME="" \
  -e API_PASSWORD="" docker_ted
```

> Es posible utilizar una imagen pública de **DockerHub**: `alexfrancow/ted-connector:latest`

Es posible desplegar el conector en **Kubernetes**, en los distintos *clouds* como: **Azure**, **Google** o **AWS**. Para ello se recomienda generar unos *secrets* para que estes no se encuentren *hardcodeados* en ningún fichero de configuración.

```bash
TWITTER_APP_KEY = ""
TWITTER_APP_SECRET = ""
ACCESS_TOKEN = ""
ACCESS_TOKEN_SECRET = ""
BEARER_TOKEN = ""
NAMESPACE=""

kubectl create secret generic ted-creds --from-literal=TWITTER_APP_KEY=$TWITTER_APP_KEY \
  --from-literal=TWITTER_APP_SECRET=$TWITTER_APP_SECRET \
  --from-literal=ACCESS_TOKEN=$ACCESS_TOKEN \
  --from-literal=ACCESS_TOKEN_SECRET=$ACCESS_TOKEN_SECRET \
  --from-literal=BEARER_TOKEN=$BEARER_TOKEN \
  --namespace=$NAMESPACE -o yaml

INDEXER_USERNAME = ""
INDEXER_PASSWORD = ""

kubectl create secret generic indexer-creds --from-literal=username=$INDEXER_USERNAME \
  --from-literal=password=$INDEXER_PASSWORD \
  --namespace=$NAMESPACE -o yaml

API_USERNAME = ""
API_PASSWORD = ""
kubectl create secret generic wazuh-api-cred --from-literal=username=$API_USERNAME \
  --from-literal=password=$API_PASSWORD \
  --namespace=$NAMESPACE -o yaml
```

Los campos del evento que se almacena en **OpenSearch/ElasticSearch** son los siguientes:

```json
{
  "properties": {
    "cve" : {
          "type" : "text",
          "fields" : {
            "keyword" : {
              "type" : "keyword",
              "ignore_above" : 256
            }
          }
      },
    "time" : {
          "type" : "date",
          "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
        },
    "id" : {
          "type" : "long"
      },
    "timestamp" : {
          "type" : "date",
	  "format": "epoch_second"
      },
    "poc": {
      "type": "text",
      "fielddata": true,
      "fields" : {
            "keyword" : {
              "type" : "text",
              "fielddata": true
            }
          }
      }
  }
}
```

> Es posible editar la función `pushCve` de la clase `Indexer` para agregar cualquier campo a mayores dentro de la variable `data`. (`src/main.py`)

## Demo

<iframe width="560" height="315" src="https://www.youtube.com/embed/Jci5mRGwuo8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Conclusiones
El módulo funciona correctamente, y es posible agregarlo en producción bajo la supervisión de un ingeniero. Agregarlos con un `timestamp` en **OpenSearch/ElasticSearch** ayuda a ver como ha evolucionado una vulnerabilidad en **Twitter** y aporta métricas interesantes. 
**No es recomendable** agregar un **alertado automático**, ya que es posible que se produzcan **demasiados falsos positivos**. Para que este nivel de **FPs** sea menor, se recomienda seguir entrenando el **modelo** de aprendizaje automático. 

<p align="center"><img src="https://raw.githubusercontent.com/alexfrancow/alexfrancow.github.io/master/images/2021-04-05-Twitter-based-Exploit-Detector/12.png" height="500" width="825" /></p>

En la anterior entrada del blog, hay un pequeño tutorial de como aumentar el dataset. 

> El **dataset**, el **modelo**, y el **notebook** se encuentran en el repositorio. `src/dataset/DATASET.csv`, `src/notebooks/NLP-TED.ipynb` y `src/models/TW_exploit_detect_NLP_Modelv1.joblib`

El *scope* del conector es limitado, ya que se centra solamente en **Twitter**. Es posible aumentarlo consumiendo de otras **APIs**, quizás **Reddit**, **GitHub** o **GitLab** y aumentar el *scope*. Para esto es necesario tener en cuenta que, sería necesario entrenar otro modelo de **Machine Learning**.